{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31192,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"pip install PyPDF2 pdfplumber transformers torch ollama requests","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-09T08:02:53.813133Z","iopub.execute_input":"2025-11-09T08:02:53.813549Z","iopub.status.idle":"2025-11-09T08:02:58.053331Z","shell.execute_reply.started":"2025-11-09T08:02:53.813524Z","shell.execute_reply":"2025-11-09T08:02:58.052200Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: PyPDF2 in /usr/local/lib/python3.11/dist-packages (3.0.1)\nRequirement already satisfied: pdfplumber in /usr/local/lib/python3.11/dist-packages (0.11.8)\nRequirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.53.3)\nRequirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\nRequirement already satisfied: ollama in /usr/local/lib/python3.11/dist-packages (0.6.0)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.32.5)\nRequirement already satisfied: pdfminer.six==20251107 in /usr/local/lib/python3.11/dist-packages (from pdfplumber) (20251107)\nRequirement already satisfied: Pillow>=9.1 in /usr/local/lib/python3.11/dist-packages (from pdfplumber) (11.3.0)\nRequirement already satisfied: pypdfium2>=4.18.0 in /usr/local/lib/python3.11/dist-packages (from pdfplumber) (5.0.0)\nRequirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from pdfminer.six==20251107->pdfplumber) (3.4.4)\nRequirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.11/dist-packages (from pdfminer.six==20251107->pdfplumber) (46.0.3)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.20.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.36.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (25.0)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.3)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2025.11.3)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.2)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\nRequirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.15.0)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.10.0)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nRequirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\nRequirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\nRequirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\nRequirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\nRequirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\nRequirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nRequirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\nRequirement already satisfied: httpx>=0.27 in /usr/local/lib/python3.11/dist-packages (from ollama) (0.28.1)\nRequirement already satisfied: pydantic>=2.9 in /usr/local/lib/python3.11/dist-packages (from ollama) (2.12.4)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests) (3.11)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests) (2025.10.5)\nRequirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27->ollama) (4.11.0)\nRequirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27->ollama) (1.0.9)\nRequirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.27->ollama) (0.16.0)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.2.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2025.3.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2022.3.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2.4.1)\nRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.9->ollama) (0.7.0)\nRequirement already satisfied: pydantic-core==2.41.5 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.9->ollama) (2.41.5)\nRequirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.9->ollama) (0.4.2)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.3)\nRequirement already satisfied: cffi>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from cryptography>=36.0.0->pdfminer.six==20251107->pdfplumber) (2.0.0)\nRequirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx>=0.27->ollama) (1.3.1)\nRequirement already satisfied: onemkl-license==2025.3.0 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2025.3.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2022.3.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=2.0.0->cryptography>=36.0.0->pdfminer.six==20251107->pdfplumber) (2.23)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->transformers) (2024.2.0)\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"import os\nimport json\nimport csv\nimport PyPDF2\nimport pdfplumber\nimport requests\nfrom typing import List, Dict, Any, Optional\nfrom transformers import pipeline\nimport ollama\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-09T08:04:54.748253Z","iopub.execute_input":"2025-11-09T08:04:54.748991Z","iopub.status.idle":"2025-11-09T08:04:54.754707Z","shell.execute_reply.started":"2025-11-09T08:04:54.748947Z","shell.execute_reply":"2025-11-09T08:04:54.753389Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"class MCQGenerator:\n    def __init__(self, model_type: str = \"openai\", api_key: str = None, model_name: str = \"gpt-4\"):\n        \"\"\"\n        Initialize the MCQ Generator with specified model.\n        \n        Args:\n            model_type: Type of model to use ('openai', 'ollama', 'huggingface')\n            api_key: API key for the service\n            model_name: Specific model name\n        \"\"\"\n        self.model_type = model_type\n        self.model_name = model_name\n        \n        if model_type == \"openai\" and api_key:\n            self.client = OpenAI(api_key=api_key)\n        elif model_type == \"huggingface\":\n            self.generator = pipeline('text-generation', model=model_name)\n        elif model_type == \"ollama\":\n            # Ensure the model is available in Ollama\n            try:\n                ollama.list()\n            except:\n                print(\"Warning: Ollama might not be running\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-09T08:05:08.037586Z","iopub.execute_input":"2025-11-09T08:05:08.038018Z","iopub.status.idle":"2025-11-09T08:05:08.044408Z","shell.execute_reply.started":"2025-11-09T08:05:08.037994Z","shell.execute_reply":"2025-11-09T08:05:08.043359Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"def extract_text_from_pdf(self, pdf_path: str, use_pdfplumber: bool = True) -> str:\n        \"\"\"\n        Extract text from PDF file.\n        \n        Args:\n            pdf_path: Path to the PDF file\n            use_pdfplumber: Whether to use pdfplumber (better for complex layouts)\n            \n        Returns:\n            Extracted text as string\n        \"\"\"\n        text = \"\"\n        \n        try:\n            if use_pdfplumber:\n                with pdfplumber.open(pdf_path) as pdf:\n                    for page in pdf.pages:\n                        page_text = page.extract_text()\n                        if page_text:\n                            text += page_text + \"\\n\"\n            else:\n                with open(pdf_path, 'rb') as file:\n                    pdf_reader = PyPDF2.PdfReader(file)\n                    for page in pdf_reader.pages:\n                        text += page.extract_text() + \"\\n\"\n                        \n        except Exception as e:\n            print(f\"Error extracting text from PDF: {e}\")\n            \n        return text","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-09T08:02:15.462565Z","iopub.execute_input":"2025-11-09T08:02:15.462878Z","iopub.status.idle":"2025-11-09T08:02:15.489237Z","shell.execute_reply.started":"2025-11-09T08:02:15.462845Z","shell.execute_reply":"2025-11-09T08:02:15.488283Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"def chunk_text(self, text: str, chunk_size: int = 1000, chunk_overlap: int = 200) -> List[str]:\n        \"\"\"\n        Split text into overlapping chunks for processing.\n        \n        Args:\n            text: Input text to chunk\n            chunk_size: Size of each chunk in characters\n            chunk_overlap: Overlap between chunks\n            \n        Returns:\n            List of text chunks\n        \"\"\"\n        chunks = []\n        start = 0\n        \n        while start < len(text):\n            end = start + chunk_size\n            chunk = text[start:end]\n            chunks.append(chunk)\n            start += chunk_size - chunk_overlap\n            \n        return chunks","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-09T08:02:15.490316Z","iopub.execute_input":"2025-11-09T08:02:15.490640Z","iopub.status.idle":"2025-11-09T08:02:15.509000Z","shell.execute_reply.started":"2025-11-09T08:02:15.490617Z","shell.execute_reply":"2025-11-09T08:02:15.507869Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"def retrieve_relevant_chunks(self, chunks: List[str], topic: str, top_k: int = 3) -> List[str]:\n        \"\"\"\n        Simple retrieval function to find relevant chunks based on topic.\n        In production, you might use vector databases like FAISS or ChromaDB.\n        \n        Args:\n            chunks: List of text chunks\n            topic: Topic to search for\n            top_k: Number of top chunks to return\n            \n        Returns:\n            List of relevant chunks\n        \"\"\"\n        # Simple keyword-based retrieval\n        topic_lower = topic.lower()\n        scored_chunks = []\n        \n        for chunk in chunks:\n            score = chunk.lower().count(topic_lower)\n            scored_chunks.append((score, chunk))\n        \n        # Sort by relevance score and return top_k\n        scored_chunks.sort(reverse=True)\n        return [chunk for _, chunk in scored_chunks[:top_k]]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-09T08:02:15.510652Z","iopub.execute_input":"2025-11-09T08:02:15.512179Z","iopub.status.idle":"2025-11-09T08:02:15.529466Z","shell.execute_reply.started":"2025-11-09T08:02:15.512129Z","shell.execute_reply":"2025-11-09T08:02:15.528442Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"def generate_mcq_with_llm(self, context: str, topic: str, num_questions: int = 1) -> List[Dict[str, Any]]:\n        \"\"\"\n        Generate MCQs using the selected LLM.\n        \n        Args:\n            context: Context text for question generation\n            topic: Specific topic for questions\n            num_questions: Number of questions to generate\n            \n        Returns:\n            List of MCQ dictionaries\n        \"\"\"\n        prompt = f\"\"\"\n        Based on the following context, generate {num_questions} multiple-choice question(s) about {topic}.\n        \n        Context:\n        {context}\n        \n        For each question, provide:\n        1. A clear question\n        2. Four options (A, B, C, D)\n        3. The correct answer (just the letter)\n        4. Brief explanatory feedback\n        \n        Format the response as a JSON array where each object has:\n        - \"question\": the question text\n        - \"options\": a dictionary with keys \"A\", \"B\", \"C\", \"D\"\n        - \"correct_answer\": the correct option letter\n        - \"feedback\": explanatory text\n        \n        Generate only the JSON array, no additional text.\n        \"\"\"\n        \n        if self.model_type == \"openai\":\n            return self._generate_with_openai(prompt)\n        elif self.model_type == \"ollama\":\n            return self._generate_with_ollama(prompt)\n        elif self.model_type == \"huggingface\":\n            return self._generate_with_huggingface(prompt)\n        else:\n            raise ValueError(f\"Unsupported model type: {self.model_type}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-09T08:02:15.530482Z","iopub.execute_input":"2025-11-09T08:02:15.531108Z","iopub.status.idle":"2025-11-09T08:02:15.546602Z","shell.execute_reply.started":"2025-11-09T08:02:15.531081Z","shell.execute_reply":"2025-11-09T08:02:15.545670Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"def _generate_with_openai(self, prompt: str) -> List[Dict[str, Any]]:\n        \"\"\"Generate MCQs using OpenAI API.\"\"\"\n        try:\n            response = self.client.chat.completions.create(\n                model=self.model_name,\n                messages=[\n                    {\"role\": \"system\", \"content\": \"You are an expert educational content creator. Generate accurate, clear multiple-choice questions.\"},\n                    {\"role\": \"user\", \"content\": prompt}\n                ],\n                temperature=0.7,\n                max_tokens=2000\n            )\n            \n            content = response.choices[0].message.content.strip()\n            return json.loads(content)\n            \n        except Exception as e:\n            print(f\"Error generating with OpenAI: {e}\")\n            return []","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-09T08:02:15.550310Z","iopub.execute_input":"2025-11-09T08:02:15.550591Z","iopub.status.idle":"2025-11-09T08:02:15.567488Z","shell.execute_reply.started":"2025-11-09T08:02:15.550571Z","shell.execute_reply":"2025-11-09T08:02:15.566632Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"def _generate_with_ollama(self, prompt: str) -> List[Dict[str, Any]]:\n        \"\"\"Generate MCQs using Ollama.\"\"\"\n        try:\n            response = ollama.generate(\n                model=self.model_name,\n                prompt=prompt,\n                options={'temperature': 0.7}\n            )\n            \n            content = response['response'].strip()\n            return json.loads(content)\n            \n        except Exception as e:\n            print(f\"Error generating with Ollama: {e}\")\n            return []","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-09T08:02:15.568365Z","iopub.execute_input":"2025-11-09T08:02:15.568657Z","iopub.status.idle":"2025-11-09T08:02:15.587556Z","shell.execute_reply.started":"2025-11-09T08:02:15.568637Z","shell.execute_reply":"2025-11-09T08:02:15.586500Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":" def _generate_with_huggingface(self, prompt: str) -> List[Dict[str, Any]]:\n        \"\"\"Generate MCQs using Hugging Face transformers.\"\"\"\n        try:\n            response = self.generator(\n                prompt,\n                max_length=2000,\n                temperature=0.7,\n                do_sample=True,\n                num_return_sequences=1\n            )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-09T08:02:15.588516Z","iopub.execute_input":"2025-11-09T08:02:15.589048Z","iopub.status.idle":"2025-11-09T08:02:15.607130Z","shell.execute_reply.started":"2025-11-09T08:02:15.589025Z","shell.execute_reply":"2025-11-09T08:02:15.605930Z"}},"outputs":[{"traceback":["\u001b[0;36m  File \u001b[0;32m\"/tmp/ipykernel_200/4266373682.py\"\u001b[0;36m, line \u001b[0;32m10\u001b[0m\n\u001b[0;31m    )\u001b[0m\n\u001b[0m     ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m incomplete input\n"],"ename":"SyntaxError","evalue":"incomplete input (4266373682.py, line 10)","output_type":"error"}],"execution_count":11},{"cell_type":"code","source":"content = response[0]['generated_text'].replace(prompt, '').strip()\n            # Extract JSON from response\n            start_idx = content.find('[')\n            end_idx = content.rfind(']') + 1\n            if start_idx != -1 and end_idx != 0:\n                json_str = content[start_idx:end_idx]\n                return json.loads(json_str)\n            return []\n            \n        except Exception as e:\n            print(f\"Error generating with Hugging Face: {e}\")\n            return []","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-09T08:02:15.607768Z","iopub.status.idle":"2025-11-09T08:02:15.608063Z","shell.execute_reply.started":"2025-11-09T08:02:15.607940Z","shell.execute_reply":"2025-11-09T08:02:15.607953Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def save_to_json(self, mcqs: List[Dict[str, Any]], filename: str):\n        \"\"\"Save MCQs to JSON file.\"\"\"\n        try:\n            with open(filename, 'w', encoding='utf-8') as f:\n                json.dump(mcqs, f, indent=2, ensure_ascii=False)\n            print(f\"MCQs saved to {filename}\")\n        except Exception as e:\n            print(f\"Error saving to JSON: {e}\")\n    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-09T08:02:15.608830Z","iopub.status.idle":"2025-11-09T08:02:15.609112Z","shell.execute_reply.started":"2025-11-09T08:02:15.608990Z","shell.execute_reply":"2025-11-09T08:02:15.609002Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def save_to_csv(self, mcqs: List[Dict[str, Any]], filename: str):\n        \"\"\"Save MCQs to CSV file.\"\"\"\n        try:\n            with open(filename, 'w', newline='', encoding='utf-8') as f:\n                writer = csv.writer(f)\n                # Write header\n                writer.writerow(['question', 'option_A', 'option_B', 'option_C', 'option_D', 'correct_answer', 'feedback'])\n                \n                for mcq in mcqs:\n                    writer.writerow([\n                        mcq['question'],\n                        mcq['options']['A'],\n                        mcq['options']['B'],\n                        mcq['options']['C'],\n                        mcq['options']['D'],\n                        mcq['correct_answer'],\n                        mcq['feedback']\n                    ])\n            print(f\"MCQs saved to {filename}\")\n        except Exception as e:\n            print(f\"Error saving to CSV: {e}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-09T08:02:15.611303Z","iopub.status.idle":"2025-11-09T08:02:15.611647Z","shell.execute_reply.started":"2025-11-09T08:02:15.611506Z","shell.execute_reply":"2025-11-09T08:02:15.611523Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def generate_moodle_format(self, mcqs: List[Dict[str, Any]]) -> str:\n        \"\"\"\n        Convert MCQs to Moodle GIFT format for easy import.\n        \n        Returns:\n            String in GIFT format\n        \"\"\"\n        gift_text = \"\"\n        for mcq in mcqs:\n            question = mcq['question'].replace('{', '\\\\{').replace('}', '\\\\}')\n            correct_option = mcq['options'][mcq['correct_answer']]\n            \n            gift_text += f\"::{question}::\"\n            gift_text += \"{\" + correct_option.replace('=', '\\\\=')\n            \n            # Add other options as wrong answers\n            for option, text in mcq['options'].items():\n                if option != mcq['correct_answer']:\n                    gift_text += f\" ~{text.replace('=', '\\\\=')}\"\n            \n            gift_text += \"}\\n\\n\"\n        \n        return gift_text","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-09T08:02:15.612719Z","iopub.status.idle":"2025-11-09T08:02:15.613060Z","shell.execute_reply.started":"2025-11-09T08:02:15.612921Z","shell.execute_reply":"2025-11-09T08:02:15.612938Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def main():\n    \"\"\"Example usage of the MCQ Generator.\"\"\"\n    \n    # Initialize the generator (choose one model type)\n    generator = MCQGenerator(\n        model_type=\"openai\",  # or \"ollama\", \"huggingface\"\n        api_key=\"your-openai-api-key\",  # Replace with your API key\n        model_name=\"gpt-4\"\n    )\n    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-09T08:02:15.614219Z","iopub.status.idle":"2025-11-09T08:02:15.614619Z","shell.execute_reply.started":"2025-11-09T08:02:15.614423Z","shell.execute_reply":"2025-11-09T08:02:15.614439Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Extract text from PDF\n    pdf_path = \"sample_document.pdf\"  # Replace with your PDF path\n    if os.path.exists(pdf_path):\n        text = generator.extract_text_from_pdf(pdf_path)\n        print(f\"Extracted {len(text)} characters from PDF\")\n        \n        # Chunk the text\n        chunks = generator.chunk_text(text)\n        print(f\"Created {len(chunks)} chunks\")\n        \n        # Retrieve relevant chunks for a topic\n        topic = \"artificial intelligence\"\n        relevant_chunks = generator.retrieve_relevant_chunks(chunks, topic)\n        context = \"\\n\".join(relevant_chunks)\n        ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-09T08:02:15.616095Z","iopub.status.idle":"2025-11-09T08:02:15.616472Z","shell.execute_reply.started":"2025-11-09T08:02:15.616277Z","shell.execute_reply":"2025-11-09T08:02:15.616293Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Generate MCQs\n        mcqs = generator.generate_mcq_with_llm(context, topic, num_questions=3)\n        \n        if mcqs:\n            print(f\"Generated {len(mcqs)} MCQs\")\n            \n            # Display generated MCQs\n            for i, mcq in enumerate(mcqs, 1):\n                print(f\"\\n--- Question {i} ---\")\n                print(f\"Q: {mcq['question']}\")\n                for option, text in mcq['options'].items():\n                    print(f\"  {option}: {text}\")\n                print(f\"Correct: {mcq['correct_answer']}\")\n                print(f\"Feedback: {mcq['feedback']}\")\n            \n            # Save to files\n            generator.save_to_json(mcqs, \"generated_mcqs.json\")\n            generator.save_to_csv(mcqs, \"generated_mcqs.csv\")\n            \n            # Generate Moodle format\n            moodle_format = generator.generate_moodle_format(mcqs)\n            with open(\"moodle_questions.gift\", \"w\", encoding=\"utf-8\") as f:\n                f.write(moodle_format)\n            print(\"\\nMoodle GIFT format saved to moodle_questions.gift\")\n            \n        else:\n            print(\"No MCQs were generated\")\n    else:\n        print(\"PDF file not found. Using sample context...\")\n        \n        # Fallback: Generate from sample context\n        sample_context = \"\"\"\n        Retrieval-Augmented Generation (RAG) is a technique that combines retrieval-based methods \n        with generative AI models. It enhances the accuracy and factual consistency of generated \n        content by first retrieving relevant information from a knowledge source, then using that \n        information to generate responses. This approach is particularly useful for applications \n        requiring up-to-date or domain-specific knowledge, as it grounds the generation in factual data.\n        \n        Large Language Models (LLMs) like GPT-4 are trained on vast amounts of text data and can \n        generate human-like responses. However, they may sometimes produce inaccurate or outdated \n        information. RAG addresses this limitation by providing the model with current, relevant \n        context from external sources before generation.\n        \"\"\"\n        \n        mcqs = generator.generate_mcq_with_llm(sample_context, \"RAG and LLMs\", num_questions=2)\n        \n        if mcqs:\n            for i, mcq in enumerate(mcqs, 1):\n                print(f\"\\n--- Question {i} ---\")\n                print(f\"Q: {mcq['question']}\")\n                for option, text in mcq['options'].items():\n                    print(f\"  {option}: {text}\")\n                print(f\"Correct: {mcq['correct_answer']}\")\n                print(f\"Feedback: {mcq['feedback']}\")\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-09T08:02:15.617514Z","iopub.status.idle":"2025-11-09T08:02:15.617858Z","shell.execute_reply.started":"2025-11-09T08:02:15.617660Z","shell.execute_reply":"2025-11-09T08:02:15.617675Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# For OpenAI\ngenerator = MCQGenerator(model_type=\"openai\", api_key=\"your-key\")\n\n# For Ollama (local models)\ngenerator = MCQGenerator(model_type=\"ollama\", model_name=\"mistral\")\n\n# For Hugging Face\ngenerator = MCQGenerator(model_type=\"huggingface\", model_name=\"microsoft/DialoGPT-medium\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-09T08:02:15.619195Z","iopub.status.idle":"2025-11-09T08:02:15.619547Z","shell.execute_reply.started":"2025-11-09T08:02:15.619378Z","shell.execute_reply":"2025-11-09T08:02:15.619394Z"}},"outputs":[],"execution_count":null}]}